{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL THE REQUIREMENTS\n",
    "\n",
    "Note also that if you're using a colab, be sure to select a Hardward Accelerator (`Runtime` -> `Change Runtime Type` -> `GPU`/`TPU`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/luchris429/purejaxrl/main/requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Gymnax MinAtar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISCUSSION\n",
    "\n",
    "The following hyperparameters were briefly tuned to do well on Gymnax's MinAtar environments. I plan to release more rigorous benchmarking, timing, and hyperparameter sweeps. Note that to run multiple in parallel, you just need to `vmap` the `train` function like in the `example_0.ipynb` notebook\n",
    "\n",
    "This should take ~45 seconds (including compilation time) on an NVIDIA A40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "import gymnax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        pi = distrax.Categorical(logits=actor_mean)\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "\n",
    "        # INIT NETWORK\n",
    "        network = ActorCritic(env.action_space(env_params).n, activation=config[\"ACTIVATION\"])\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi, value = network.apply(train_state.params, last_obs)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0,0,0,None))(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_state, env_state, last_obs, rng = runner_state\n",
    "            _, last_val = network.apply(train_state.params, last_obs)\n",
    "\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of envs\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "\n",
    "            runner_state = (train_state, env_state, last_obs, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train\n",
    "# !pip3 install distrax, gymnax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jax-source/jax/_src/ops/scatter.py:94: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
      "/opt/jax-source/jax/_src/ops/scatter.py:94: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
      "/opt/jax-source/jax/_src/ops/scatter.py:94: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
      "/opt/jax-source/jax/_src/ops/scatter.py:94: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"LR\": 5e-3,\n",
    "    \"NUM_ENVS\": 64,\n",
    "    \"NUM_STEPS\": 128,\n",
    "    \"TOTAL_TIMESTEPS\": 1e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 8,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.01,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"ENV_NAME\": \"SpaceInvaders-MinAtar\",\n",
    "    \"ANNEAL_LR\": True,\n",
    "}\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "train_jit = jax.jit(make_train(config))\n",
    "out = train_jit(rng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(out[\"metrics\"][\"returned_episode_returns\"].mean(-1).reshape(-1))\n",
    "plt.xlabel(\"Updates\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISCUSSION\n",
    "\n",
    "Implementing continuous action PPO is just a few lines of code change. The chosen hyperparameters and implementation are designed to trade off wallclock time, performance, simplicity, and similarity to OpenAI (and CleanRL's) original PPO implementation.\n",
    "\n",
    "Some key differences to CleanRL's ppo_continuous_action implementation:\n",
    "\n",
    "1. The hyperparameters are different -- we trade off sample efficiency for wallclock time. Our considerations are similar to those of CleanRL's Isaac Gym implementation.\n",
    "\n",
    "3. Also following CleanRL's Isaac Gym implementation, the network size is slightly larger \n",
    "\n",
    "Our implementation also differs significantly from Brax's, which has different hyperparameters for different tasks and many non-standard code-level implementation details.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Wrappers\n",
    "\n",
    "We copy over the relevant content from `wrappers.py`. This is a temporary solution while I figure out how users best want to trade off modularizing and code accessibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import chex\n",
    "import numpy as np\n",
    "from flax import struct\n",
    "from functools import partial\n",
    "from typing import Optional, Tuple, Union, Any\n",
    "from gymnax.environments import environment, spaces\n",
    "from gymnax.wrappers.purerl import GymnaxWrapper\n",
    "from brax import envs\n",
    "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
    "\n",
    "class BraxGymnaxWrapper:\n",
    "    def __init__(self, env_name, backend=\"positional\"):\n",
    "        env = envs.get_environment(env_name=env_name, backend=backend)\n",
    "        env = EpisodeWrapper(env, episode_length=1000, action_repeat=1)\n",
    "        env = AutoResetWrapper(env)\n",
    "        self._env = env\n",
    "        self.action_size = env.action_size\n",
    "        self.observation_size = (env.observation_size,)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        state = self._env.reset(key)\n",
    "        return state.obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        next_state = self._env.step(state, action)\n",
    "        return next_state.obs, next_state, next_state.reward, next_state.done > 0.5, {}\n",
    "\n",
    "    def observation_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=-jnp.inf,\n",
    "            high=jnp.inf,\n",
    "            shape=(self._env.observation_size,),\n",
    "        )\n",
    "\n",
    "    def action_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(self._env.action_size,),\n",
    "        )\n",
    "\n",
    "class ClipAction(GymnaxWrapper):\n",
    "    def __init__(self, env, low=-1.0, high=1.0):\n",
    "        super().__init__(env)\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        \"\"\"TODO: In theory the below line should be the way to do this.\"\"\"\n",
    "        # action = jnp.clip(action, self.env.action_space.low, self.env.action_space.high)\n",
    "        action = jnp.clip(action, self.low, self.high)\n",
    "        return self._env.step(key, state, action, params)\n",
    "\n",
    "class VecEnv(GymnaxWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.reset = jax.vmap(self._env.reset, in_axes=(0, None))\n",
    "        self.step = jax.vmap(self._env.step, in_axes=(0, 0, 0, None))\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizeVecObsEnvState:\n",
    "    mean: jnp.ndarray\n",
    "    var: jnp.ndarray\n",
    "    count: float\n",
    "    env_state: environment.EnvState\n",
    "\n",
    "class NormalizeVecObservation(GymnaxWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=jnp.zeros_like(obs),\n",
    "            var=jnp.ones_like(obs),\n",
    "            count=1e-4,\n",
    "            env_state=state,\n",
    "        )\n",
    "        batch_mean = jnp.mean(obs, axis=0)\n",
    "        batch_var = jnp.var(obs, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            env_state=state.env_state,\n",
    "        )\n",
    "\n",
    "        return (obs - state.mean) / jnp.sqrt(state.var + 1e-8), state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, env_state, reward, done, info = self._env.step(key, state.env_state, action, params)\n",
    "\n",
    "        batch_mean = jnp.mean(obs, axis=0)\n",
    "        batch_var = jnp.var(obs, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        return (obs - state.mean) / jnp.sqrt(state.var + 1e-8), state, reward, done, info\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizeVecRewEnvState:\n",
    "    mean: jnp.ndarray\n",
    "    var: jnp.ndarray\n",
    "    count: float\n",
    "    return_val: float\n",
    "    env_state: environment.EnvState\n",
    "\n",
    "class NormalizeVecReward(GymnaxWrapper):\n",
    "\n",
    "    def __init__(self, env, gamma):\n",
    "        super().__init__(env)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        batch_count = obs.shape[0]\n",
    "        state = NormalizeVecRewEnvState(\n",
    "            mean=0.0,\n",
    "            var=1.0,\n",
    "            count=1e-4,\n",
    "            return_val=jnp.zeros((batch_count,)),\n",
    "            env_state=state,\n",
    "        )\n",
    "        return obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, env_state, reward, done, info = self._env.step(key, state.env_state, action, params)\n",
    "        return_val = (state.return_val * self.gamma * (1 - done) + reward)\n",
    " \n",
    "        batch_mean = jnp.mean(return_val, axis=0)\n",
    "        batch_var = jnp.var(return_val, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecRewEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            return_val=return_val,\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        return obs, state, reward / jnp.sqrt(state.var + 1e-8), done, info\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run PPO Brax\n",
    "\n",
    "The below code should run in ~2.5 minutes (including compilation time) on an NVIDIA A40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_logtstd = self.param(\"log_std\", nn.initializers.zeros, (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(actor_mean, jnp.exp(actor_logtstd))\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    env, env_params = BraxGymnaxWrapper(config[\"ENV_NAME\"]), None\n",
    "    env = LogWrapper(env)\n",
    "    env = ClipAction(env)\n",
    "    env = VecEnv(env)\n",
    "    if config[\"NORMALIZE_ENV\"]:\n",
    "        env = NormalizeVecObservation(env)\n",
    "        env = NormalizeVecReward(env, config[\"GAMMA\"])\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "        # INIT NETWORK\n",
    "        network = ActorCritic(\n",
    "            env.action_space(env_params).shape[0], activation=config[\"ACTIVATION\"]\n",
    "        )\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi, value = network.apply(train_state.params, last_obs)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = env.step(rng_step, env_state, action, env_params)\n",
    "                transition = Transition(\n",
    "                    done, action, value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_state, env_state, last_obs, rng = runner_state\n",
    "            _, last_val = network.apply(train_state.params, last_obs)\n",
    "\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of envs\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "            if config.get(\"DEBUG\"):\n",
    "                def callback(info):\n",
    "                    return_values = info[\"returned_episode_returns\"][info[\"returned_episode\"]]\n",
    "                    timesteps = info[\"timestep\"][info[\"returned_episode\"]] * config[\"NUM_ENVS\"]\n",
    "                    for t in range(len(timesteps)):\n",
    "                        print(f\"global step={timesteps[t]}, episodic return={return_values[t]}\")\n",
    "                jax.debug.callback(callback, metric)\n",
    "\n",
    "            runner_state = (train_state, env_state, last_obs, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"LR\": 3e-4,\n",
    "    \"NUM_ENVS\": 2048,\n",
    "    \"NUM_STEPS\": 10,\n",
    "    \"TOTAL_TIMESTEPS\": 5e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 32,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.0,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"tanh\",\n",
    "    \"ENV_NAME\": \"hopper\",\n",
    "    \"ANNEAL_LR\": False,\n",
    "    \"NORMALIZE_ENV\": True,\n",
    "    \"DEBUG\": True,\n",
    "}\n",
    "rng = jax.random.PRNGKey(30)\n",
    "train_jit = jax.jit(make_train(config))\n",
    "out = train_jit(rng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(out[\"metrics\"][\"returned_episode_returns\"].mean(-1).reshape(-1))\n",
    "plt.xlabel(\"Updates\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run DPO Brax\n",
    "\n",
    "This implements Discovered Policy Optimisation (DPO) from https://arxiv.org/abs/2210.05639\n",
    "\n",
    "Notably, this re-implementation uses the discovered policy objective that was trained on a completely different PPO implementation with several differing design details, yet it still transfers to this version and outperforms PPO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_logtstd = self.param(\"log_std\", nn.initializers.zeros, (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(actor_mean, jnp.exp(actor_logtstd))\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "\n",
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    env, env_params = BraxGymnaxWrapper(config[\"ENV_NAME\"]), None\n",
    "    env = LogWrapper(env)\n",
    "    env = ClipAction(env)\n",
    "    env = VecEnv(env)\n",
    "    if config[\"NORMALIZE_ENV\"]:\n",
    "        env = NormalizeVecObservation(env)\n",
    "        env = NormalizeVecReward(env, config[\"GAMMA\"])\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "        # INIT NETWORK\n",
    "        network = ActorCritic(\n",
    "            env.action_space(env_params).shape[0], activation=config[\"ACTIVATION\"]\n",
    "        )\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi, value = network.apply(train_state.params, last_obs)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = env.step(rng_step, env_state, action, env_params)\n",
    "                transition = Transition(\n",
    "                    done, action, value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_state, env_state, last_obs, rng = runner_state\n",
    "            _, last_val = network.apply(train_state.params, last_obs)\n",
    "\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        alpha = config[\"DPO_ALPHA\"]\n",
    "                        beta = config[\"DPO_BETA\"]\n",
    "                        log_diff = log_prob - traj_batch.log_prob\n",
    "                        ratio = jnp.exp(log_diff)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        is_pos = (gae >= 0.0).astype(\"float32\")\n",
    "                        r1 = ratio - 1.0\n",
    "                        drift1 = nn.relu(r1 * gae - alpha * nn.tanh(r1 * gae / alpha))\n",
    "                        drift2 = nn.relu(\n",
    "                            log_diff * gae - beta * nn.tanh(log_diff * gae / beta)\n",
    "                        )\n",
    "                        drift = drift1 * is_pos + drift2 * (1 - is_pos)\n",
    "                        loss_actor = -(ratio * gae - drift).mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of envs\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "            if config.get(\"DEBUG\"):\n",
    "                def callback(info):\n",
    "                    return_values = info[\"returned_episode_returns\"][info[\"returned_episode\"]]\n",
    "                    timesteps = info[\"timestep\"][info[\"returned_episode\"]] * config[\"NUM_ENVS\"]\n",
    "                    for t in range(len(timesteps)):\n",
    "                        print(f\"global step={timesteps[t]}, episodic return={return_values[t]}\")\n",
    "                jax.debug.callback(callback, metric)\n",
    "\n",
    "            runner_state = (train_state, env_state, last_obs, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"LR\": 3e-4,\n",
    "    \"NUM_ENVS\": 2048,\n",
    "    \"NUM_STEPS\": 10,\n",
    "    \"TOTAL_TIMESTEPS\": 5e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 32,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"DPO_ALPHA\": 2.0,\n",
    "    \"DPO_BETA\": 0.6,\n",
    "    \"ENT_COEF\": 0.0,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"tanh\",\n",
    "    \"ENV_NAME\": \"hopper\",\n",
    "    \"ANNEAL_LR\": False,\n",
    "    \"NORMALIZE_ENV\": True,\n",
    "    \"DEBUG\": True,\n",
    "}\n",
    "rng = jax.random.PRNGKey(30)\n",
    "train_jit = jax.jit(make_train(config))\n",
    "out = train_jit(rng)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: PPO Jumanji"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: PPO PgX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
